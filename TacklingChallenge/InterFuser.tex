Autonomous driving systems must provide a thorough comprehension of the environment and prioritize safety, which is made challenging by the requirement to analyze data from multiple types of sensors. Conventional methods of combining sensors, such as matching geometric features and basic concatenation, are unable to accurately represent the intricate relationships between diverse sensor modalities. These constraints lead to weaknesses in crucial traffic scenarios, such as the abrupt presence of people or vehicles violating traffic signals.

InterFuser \cite{shao2023safety} tackles these difficulties by utilizing sophisticated methods to efficiently combine data from various sensors. Previous methodologies had challenges in terms of scalability and interpretability, frequently resulting in compromised safety and dependability in autonomous driving systems. InterFuser utilizes a combination of convolutional neural networks and a transformer model to effectively analyze and merge sensor input. This advanced architecture significantly improves the system's capability to manage various intricate driving situations.

\subsubsection{Brief Explanation of InterFuser}

\paragraph{Input and Output Representations.} InterFuser employs a set of four sensors to acquire a thorough comprehension of the scene: three RGB cameras (located on the left, front, and right sides) and one LiDAR sensor. These sensors offer additional perspectives of the surroundings. The RGB cameras record photos from various angles, while a supplementary focusing-view image is generated by extracting the central portion of the front RGB image to specifically highlight distant traffic lights. The LiDAR point cloud data is transformed into a 3-bin histogram using a 3-dimensional Bird's Eye View (BEV) grid. This transformation produces a two-channel LiDAR BEV projection picture input.

InterFuser produces two distinct categories of outputs: safety-insensitive and safety-sensitive. The safety-insensitive outputs consist of a path prediction of 10 waypoints for the ego vehicle to adhere to. The safety-critical outputs consist of an item density map and information on traffic rules. The object density map offers comprehensive data on potential objects in the surrounding area, while the traffic rule information encompasses the current state of traffic lights, the presence of stop signs, and if the vehicle is located at a junction.


\paragraph{Model Architecture.}

The architecture of InterFuser is specifically engineered to effectively manage the intricate spatial and modal intricacies of sensors used in autonomous driving. The system is composed of three primary elements: a convolutional neural network (CNN) as the foundation, a transformer encoder, and a transformer decoder. The updated ResNet architecture is utilized to process each sensor input and extract feature maps. The feature maps capture essential visual and spatial details from the pictures and LiDAR data, making them ready for subsequent processing in the transformer phases.

The feature maps that have been recovered are then inputted into a transformer encoder. This encoder utilizes a sequence of self-attention techniques to effectively combine information from various sensors and perspectives. This stage captures intricate interdependencies and correlations among different elements, hence augmenting the model's capacity to comprehend the scene in a contextual manner.

The transformer decoder utilizes the incorporated characteristics from the encoder to provide predictions and interpretable outputs. The system utilizes many inquiries that pertain to different facets of driving, including waypoints, traffic density, and adherence to traffic regulations. The implementation of a multi-headed attention method enables the system to selectively concentrate on pertinent attributes for each individual driving decision, hence augmenting the precision and safety of the results.

The transformer decoder is accompanied by three concurrent prediction modules for forecasting the waypoints, object density map, and traffic rule information. The waypoints are forecasted using a single-layer GRU model with autoregressive capabilities. The density map of the object is anticipated by passing the matching embeddings from the transformer decoder through a multi-layer perceptron (MLP) for transformation. The traffic rule information is estimated using a solitary linear constraints.


\paragraph{Safety Controller Leveraging Intermediate Interpretable Features.}

The safety controller is a crucial component of InterFuser, utilizing the intermediate interpretable features produced by the transformer decoder. It utilizes these characteristics to improve driving selections by guaranteeing that all activities are limited inside secure operational limitations.

The safety controller utilizes a two-threshold criterion to identify objects and applies dynamic propagation with a moving average to anticipate trajectories. This method guarantees the reliable identification and anticipation of moving entities and infrequent occurrences. The controller modifies the vehicle's path and velocity by utilizing the object density map and traffic rule information. It promotes safe driving by consistently keeping a safe distance from adjacent objects and strictly following traffic restrictions.

The objective of solving a linear programming optimization issue is to find the optimal velocity that maximizes driving efficiency while maintaining safety. This entails analyzing the trajectories of nearby objects and making appropriate adjustments to the vehicle's movements. InterFuser enhances the safety and dependability of autonomous driving systems by integrating sophisticated approaches, establishing itself as a prominent solution in the industry.

\subsubsection{Limitations}

Although InterFuser achieves good Driving Score, it does have several limits and areas that can be improved upon in the future. The system persists in encountering traffic violations, with instances of failure stemming from problems such as imprecise anticipation of the direction of other vehicles, miscalculating the speed of bicycles, or the inability to detect pedestrians who have fallen.

The current solution employs a two-threshold criterion to detect objects and use dynamic propagation with a moving average to predict trajectories. Although these strategies are adequate for present standards, they may not be optimal for more intricate and diverse driving conditions. 

Furthermore, the utilization of numerous sensors and intricate fusion mechanisms poses difficulties in relation to processing resources and the ability to scale in real-time, particularly when deployed in locations with limited resources. It is imperative to tackle these difficulties in order to ensure the effectiveness of practical implementations.