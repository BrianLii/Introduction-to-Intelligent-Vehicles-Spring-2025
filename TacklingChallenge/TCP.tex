End-to-end autonomous driving methods, which directly map raw sensor data to a planned trajectory
or low-level control actions, show the virtue of simplicity. The output prediction of the model for end-to-end autonomous driving generally falls into two forms: trajectory/waypoints and direct control actions. 

For methods that plan trajectory, additional controllers needed as a subsequent step to convert the planned trajectory into control signals. One potential supremacy of trajectory-based prediction is that it actually considers a relatively longer time horizon into the future. However, turning the trajectory into control actions so that the vehicle could follow the planned trajectory is not trivial. The industry usually adopts sophisticated control algorithms to achieve reliable trajectory-following performance. Simple PID controllers may perform worse in situations such as taking a big turn or starting at the red light due to the inertial problem. 

On the other hand, for control-based methods, the control signals are directly optimized. Nevertheless, their focus on the current step may cause deferred reactions to avoid potential collisions with other moving agents. The independence between the control predictions of different steps also makes the actions of the vehicle more unstable or discontinuous.

How to combine the strengths of both trajectory-based and control-based methods remains an open and active area of research in autonomous driving. One promising approach involves a hybrid framework that leverages the advantages of both techniques to achieve better overall performance. Wu et al. (2022) proposed TCP \cite{wu2022trajectory} framework, packing these two branches into a unified framework.

\subsubsection{Brief Explanation of TCP framework}

\paragraph{Input and Output Representations.} TCP framework takes as input the sensor signal from a monocular camera, the vehicle speed, and high-level navigation information that includes a discrete navigation command and target coordinates from the global planner. The model outputs control signals, which consist of longitudinal controls---throttle and brake---and a lateral control signal, steer.

\paragraph{Model Architecture.} The whole architecture of TCP framework is comprised of an input encoding stage and two parallel subsequent branches. The input image goes through a CNN based image encoder, to generate a feature map \textbf{F}. In the meantime, an MLP based encoder takes the speed and the navigation information and outputs measurement feature $\textbf{j}_\text{m}$. The encoded features are shared by both the trajectory-based and control-based prediction branches. The predictions from these two branches are then merged using a situation-based fusion strategy to produce the final control signal.

\paragraph{The trajectory planning branch} first generates a planned trajectory comprised of waypoints at $K$ steps for the agent to follow, and then the trajectory is processed by low-level controllers to get the control actions. The image feature map \(\textbf{F}\) is average pooled and concatenated with the measurement feature \(\textbf{j}_\text{m}\). This combined feature is then fed into a GRU, which auto-regressively generates future waypoints to form the planned trajectory. With the planned trajectory, the magnitudes of the vectors between consecutive waypoints represent the desired speed and are sent to PID controllers to get throttle, brake, and steer actions.

\paragraph{The multi-step control prediction branch} in TCP framework addresses the limitations of prior control-based models by introducing a novel approach. Unlike previous methods, which rely solely on the current input and IID assumption, TCP predicts multiple future actions to accommodate sequential decision-making in closed-loop tests, where historical actions affect future states. To overcome the challenge of predicting future actions with only current sensor inputs, a temporal module leveraging GRU is employed. This module captures dynamic environmental information, such as object motion and traffic changes, by auto-regressively feeding predicted actions into GRU. 

\paragraph{Trajectory-Guided Attention.} To enhance spatial consistency and incorporate crucial static details like curbs and lanes, the control branch is guided by the trajectory branch, ensuring attention is focused on relevant regions of the input image at each future time step. TCP achieves this by learning an attention map that extracts significant information from the encoded feature map $\textbf{F}$. At each time step $t$, this attention map is calculated using the hidden states of the GRU from both the control and trajectory branches through an MLP. The map aggregates the feature map \(\textbf{F}\), combining it with the control branch's hidden states to create an informative representation, which contains both static and dynamic environmental information. This representation is then fed into a policy head, shared across all time steps, to predict the corresponding control action.

Finally, to further leverage the advantages of both trajectory and control branch predictions, a situation-based fusion strategy was adopted. When the ego vehicle is turning, the control branch is more advantageous; therefore, weights of \(\alpha\) and \(1-\alpha\) are assigned to the control branch and trajectory predictions, respectively. Otherwise, the weights are reversed, with \(1-\alpha\) assigned to the trajectory prediction and \(\alpha\) to the control branch prediction.

\subsubsection{Limitations}
The TCP framework is an innovative approach that combines the strengths of both trajectory-based and control-based methods to achieve a more effective driving model. However, despite its promising results, the framework has several notable limitations that may impede its broader applicability and optimal performance.

\paragraph{Multi-Sensor Integration.} One significant limitation of the TCP framework is its reliance on GRU to handle temporal dependencies. While GRUs are effective for such scenario, their applicability becomes limited in scenarios involving multiple sensor inputs. The current GRU implementation in TCP is primarily designed to handle monocular camera input, which constrains its ability to integrate and process multi-sensor data effectively. Autonomous vehicles often rely on a variety of sensors, such as LiDAR and radar, in addition to cameras, to obtain a comprehensive understanding of the environment. The TCP framework's inability to seamlessly incorporate these diverse sensor inputs limits its potential for achieving better performance in complex driving scenarios.

\paragraph{Trajectory-Guided Attention Generalization.} 
Another limitation of the TCP framework is its approach to trajectory-guided attention. While adopting various trajectory-based methods could significantly enhance trajectory signals, the current design of TCP's attention mechanism is tightly integrated with its own trajectory prediction model. This makes it challenging to incorporate alternative trajectory-based approaches that might perform better. Consequently, this lack of generalization restricts the framework's adaptability and its potential for optimization by integrating more advanced or complementary trajectory prediction techniques.

\paragraph{Situation-Base Fusion.} The experimental results in the paper underscore a significant limitation: the use of a fixed constant hyperparameter \(\alpha\) in situation-based fusion. This parameter controls the weighting between the control branch and trajectory branch predictions across different driving scenarios. However, a fixed \(\alpha\) can lead to suboptimal results because the ideal weighting may change with different driving contexts. For example, various road conditions and traffic densities might necessitate different \(\alpha\) values for optimal performance. Adopting a more dynamic and adaptable approach, where \(\alpha\) is adjusted in real-time based on contextual cues, might improve the framework's responsiveness and effectiveness in diverse driving situations. Thus, the current rigidity of this parameter setting limits the potential for optimal performance across varying scenarios.